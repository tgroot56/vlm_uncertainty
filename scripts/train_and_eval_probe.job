#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --job-name=train_eval_probe
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=9
#SBATCH --time=00:05:00
#SBATCH --output=slurm_outputs/train_eval_probe_%A.out

module purge
module load 2025
module load Anaconda3/2025.06-1

# Navigate to the project directory
cd $HOME/vlm_uncertainty/

source activate llava-experiments

# Load hf datasets and models on scratch shared
export HF_HOME=/scratch-shared/$USER/hf_home
export HF_DATASETS_CACHE=/scratch-shared/$USER/hf_datasets
export TRANSFORMERS_CACHE=/scratch-shared/$USER/hf_transformers

mkdir -p "$HF_HOME" "$HF_DATASETS_CACHE" "$TRANSFORMERS_CACHE"

#set variables
DATA_PATH="outputs/supervised_datasets/imagenet-r/llava-hf/llava-1.5-7b-hf/run_b8e74dc670/supervision_dataset.pt"
FEATURES="answer_gen_neglogp_mean answer_gen_entropy_mean"
MODEL_TYPE="linear"
HIDDEN_DIMS="256 128"
NUM_EPOCHS=100
BATCH_SIZE=32

srun python -m src.cli.train \
  --data_path $DATA_PATH \
  --features $FEATURES \
  --model_type $MODEL_TYPE \
  --hidden_dims $HIDDEN_DIMS \
  --num_epochs $NUM_EPOCHS \
  --batch_size $BATCH_SIZE
